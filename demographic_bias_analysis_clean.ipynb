{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e611e2da-a81d-4a8c-bf01-9a402e1b46b0",
   "metadata": {},
   "source": [
    "Imports + config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0443a06-7c44-411f-b410-409943b7d842",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgetsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\debiasing-and-improving-the-robustness-of-clip-like-models\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "   ---------------------------------------- 0.0/914.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 914.9/914.9 kB 8.4 MB/s  0:00:00\n",
      "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 11.2 MB/s  0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   ------------- -------------------------- 1/3 [jupyterlab_widgets]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Debiasing-and-Improving-the-Robustness-of-CLIP-like-Models\\\\.venv\\\\share\\\\jupyter\\\\labextensions\\\\@jupyter-widgets\\\\jupyterlab-manager\\\\static\\\\vendors-node_modules_d3-color_src_color_js-node_modules_d3-format_src_defaultLocale_js-node_m-09b215.2643c43f22ad111f4f82.js'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7613e36-3b37-4b1e-967b-370d4ea64312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"code/fairface\")  # pour importer le modèle fairface si besoin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5c1ae-af04-4b51-81ee-ff6058d50bae",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cfd8c06-7e11-484c-ac04-2e667aaacca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "CSV_IN = \"data/laion_big_light_tau1_0.2989.csv\"\n",
    "CSV_OUT = \"data/laion_big_light_tau1_0.2989_fairface.csv\"\n",
    "\n",
    "FAIRFACE_MODEL_PATH = \"code/fairface/models/fairface_alldata_4race_20191111.pt\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d4be3-de38-46db-acce-5eb4f78792b1",
   "metadata": {},
   "source": [
    "Load data + sanity check images exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a73e256-a62e-4f85-b23d-706bca65787c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 107251\n",
      "['image_path', 'caption', 'width', 'height', 'similarity', 'punsafe', 'pwatermark', 'aesthetic_score', 'caption_len_words', 'exists', 'keep_quality', 'clipscore']\n",
      "Exists %: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>similarity</th>\n",
       "      <th>punsafe</th>\n",
       "      <th>pwatermark</th>\n",
       "      <th>aesthetic_score</th>\n",
       "      <th>caption_len_words</th>\n",
       "      <th>exists</th>\n",
       "      <th>keep_quality</th>\n",
       "      <th>clipscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>src/data/data/laion_aesthetic_subset_big\\00000...</td>\n",
       "      <td>Photo pour Japanese pagoda and old house in Ky...</td>\n",
       "      <td>450.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.345947</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.034099</td>\n",
       "      <td>6.526204</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.335841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>src/data/data/laion_aesthetic_subset_big\\00000...</td>\n",
       "      <td>San Pedro: One Of Mother Nature's Most Powerfu...</td>\n",
       "      <td>467.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>0.308873</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.102705</td>\n",
       "      <td>6.749783</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.300125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  src/data/data/laion_aesthetic_subset_big\\00000...   \n",
       "1  src/data/data/laion_aesthetic_subset_big\\00000...   \n",
       "\n",
       "                                             caption  width  height  \\\n",
       "0  Photo pour Japanese pagoda and old house in Ky...  450.0   297.0   \n",
       "1  San Pedro: One Of Mother Nature's Most Powerfu...  467.0   369.0   \n",
       "\n",
       "   similarity   punsafe  pwatermark  aesthetic_score  caption_len_words  \\\n",
       "0    0.345947  0.000541    0.034099         6.526204                 16   \n",
       "1    0.308873  0.000612    0.102705         6.749783                 14   \n",
       "\n",
       "   exists  keep_quality  clipscore  \n",
       "0    True          True   0.335841  \n",
       "1    True          True   0.300125  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_IN)\n",
    "print(\"Rows:\", len(df))\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# verify paths exist\n",
    "exists = df[\"image_path\"].apply(os.path.exists).mean() * 100\n",
    "print(f\"Exists %: {exists:.2f}%\")\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b57b8-d710-4f83-b2b7-260ac7f358fe",
   "metadata": {},
   "source": [
    "Face detector (MTCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed53f469-c7be-494a-9ecf-adf547d62153",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35afac8-2d24-4485-91d4-098dcc4fe164",
   "metadata": {},
   "source": [
    "FairFace model loader (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4995fdc-4cdb-4109-a186-67a20681b527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairFace model loaded ✔️\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "# FairFace uses a ResNet34 classifier\n",
    "weights = ResNet34_Weights.DEFAULT\n",
    "fairface_model = resnet34(weights=weights)\n",
    "fairface_model.fc = torch.nn.Linear(fairface_model.fc.in_features, 18)  # 18 classes output (gender+age+race encoded)\n",
    "state = torch.load(FAIRFACE_MODEL_PATH, map_location=DEVICE)\n",
    "fairface_model.load_state_dict(state, strict=False)\n",
    "fairface_model = fairface_model.to(DEVICE).eval()\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "print(\"FairFace model loaded ✔️\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a2010-856f-4cb9-8a39-8560d38e5c28",
   "metadata": {},
   "source": [
    "Helpers: decode outputs + map race→skin tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e8e246e-8ee9-48f9-8f08-96d2d32ccd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDER = [\"Male\", \"Female\"]\n",
    "AGE = [\"0-2\", \"3-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70+\"]\n",
    "RACE = [\"White\", \"Black\", \"Latino_Hispanic\", \"East Asian\", \"Southeast Asian\", \"Indian\", \"Middle Eastern\"]\n",
    "\n",
    "def race_to_skin_tone(r):\n",
    "    # approximation \"skin tone\" en 3 classes (coarse)\n",
    "    # (c'est une proxy, à expliquer dans le rapport)\n",
    "    if r in [\"White\", \"East Asian\", \"Southeast Asian\"]:\n",
    "        return \"Light\"\n",
    "    if r in [\"Latino_Hispanic\", \"Middle Eastern\", \"Indian\"]:\n",
    "        return \"Medium\"\n",
    "    if r in [\"Black\"]:\n",
    "        return \"Dark\"\n",
    "    return None\n",
    "\n",
    "def decode_fairface(logits):\n",
    "    # logits shape [18]\n",
    "    # structure standard fairface:\n",
    "    # 2 gender + 9 age + 7 race = 18\n",
    "    g = torch.argmax(logits[0:2]).item()\n",
    "    a = torch.argmax(logits[2:11]).item()\n",
    "    r = torch.argmax(logits[11:18]).item()\n",
    "    return GENDER[g], AGE[a], RACE[r]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835d409-cb7e-45a1-9d19-d02eb2b9dc91",
   "metadata": {},
   "source": [
    "Main inference loop (sample d’abord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b4eb1b-9e52-4ab9-b895-dc8b768bc0fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 300/300 [01:20<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detected %: 36.333333333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>face_detected</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "      <th>race</th>\n",
       "      <th>skin_tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Male</td>\n",
       "      <td>0-2</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>Male</td>\n",
       "      <td>40-49</td>\n",
       "      <td>Black</td>\n",
       "      <td>Dark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>Male</td>\n",
       "      <td>0-2</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   face_detected gender age_group        race skin_tone\n",
       "0          False   None      None        None      None\n",
       "1           True   Male       0-2  East Asian     Light\n",
       "2          False   None      None        None      None\n",
       "3          False   None      None        None      None\n",
       "4           True   Male     40-49       Black      Dark\n",
       "5           True   Male       0-2  East Asian     Light\n",
       "6          False   None      None        None      None\n",
       "7          False   None      None        None      None\n",
       "8          False   None      None        None      None\n",
       "9          False   None      None        None      None"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = df.sample(n=300, random_state=0).reset_index(drop=True)\n",
    "\n",
    "out_rows = []\n",
    "for i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "    path = row[\"image_path\"]\n",
    "    cap = row.get(\"caption\", \"\")\n",
    "\n",
    "    face_detected = False\n",
    "    gender = None\n",
    "    age_group = None\n",
    "    skin_tone = None\n",
    "    race = None\n",
    "\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            face_detected = True\n",
    "\n",
    "            # take largest face\n",
    "            areas = [(b[2]-b[0])*(b[3]-b[1]) for b in boxes]\n",
    "            j = int(np.argmax(areas))\n",
    "            x1, y1, x2, y2 = boxes[j].astype(int)\n",
    "\n",
    "            face = img.crop((x1, y1, x2, y2))\n",
    "            x = transform(face).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = fairface_model(x).squeeze(0)\n",
    "                gender, age_group, race = decode_fairface(logits)\n",
    "                skin_tone = race_to_skin_tone(race)\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    out_rows.append({\n",
    "        **row.to_dict(),\n",
    "        \"face_detected\": face_detected,\n",
    "        \"gender\": gender,\n",
    "        \"age_group\": age_group,\n",
    "        \"race\": race,\n",
    "        \"skin_tone\": skin_tone\n",
    "    })\n",
    "\n",
    "sample_out = pd.DataFrame(out_rows)\n",
    "print(\"Face detected %:\", sample_out[\"face_detected\"].mean()*100)\n",
    "sample_out[[\"face_detected\",\"gender\",\"age_group\",\"race\",\"skin_tone\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f5528-49ba-4ac7-87b7-c20aeee93adf",
   "metadata": {},
   "source": [
    "Run full dataset (avec checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01960fc-a8d6-42ee-a839-c74caa9ac067",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m CHECKPOINT_CSV \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/laion_big_light_tau1_0.2989_fairface_checkpoint.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Resume if checkpoint exists\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(CHECKPOINT_CSV):\n\u001b[0;32m      5\u001b[0m     done \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(CHECKPOINT_CSV)\n\u001b[0;32m      6\u001b[0m     done_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(done[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_CSV = \"data/laion_big_light_tau1_0.2989_fairface_checkpoint.csv\"\n",
    "\n",
    "# Resume if checkpoint exists\n",
    "if os.path.exists(CHECKPOINT_CSV):\n",
    "    done = pd.read_csv(CHECKPOINT_CSV)\n",
    "    done_paths = set(done[\"image_path\"].astypon e(str).tolist())\n",
    "    print(\"Resume: already processed:\", len(done_paths))\n",
    "else:\n",
    "    done = pd.DataFrame()\n",
    "    done_paths = set()\n",
    "\n",
    "out_rows = []\n",
    "save_every = 10000\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    path = str(row[\"image_path\"])\n",
    "    if path in done_paths:\n",
    "        continue\n",
    "\n",
    "    face_detected = False\n",
    "    gender = None\n",
    "    age_group = None\n",
    "    skin_tone = None\n",
    "    race = None\n",
    "\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            face_detected = True\n",
    "            areas = [(b[2]-b[0])*(b[3]-b[1]) for b in boxes]\n",
    "            j = int(np.argmax(areas))\n",
    "            x1, y1, x2, y2 = boxes[j].astype(int)\n",
    "\n",
    "            face = img.crop((x1, y1, x2, y2))\n",
    "            x = transform(face).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = fairface_model(x).squeeze(0)\n",
    "                gender, age_group, race = decode_fairface(logits)\n",
    "                skin_tone = race_to_skin_tone(race)\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    out_rows.append({\n",
    "        **row.to_dict(),\n",
    "        \"face_detected\": face_detected,\n",
    "        \"gender\": gender,\n",
    "        \"age_group\": age_group,\n",
    "        \"race\": race,\n",
    "        \"skin_tone\": skin_tone\n",
    "    })\n",
    "\n",
    "    if len(out_rows) >= save_every:\n",
    "        chunk = pd.DataFrame(out_rows)\n",
    "        out_rows = []\n",
    "\n",
    "        if len(done) == 0:\n",
    "            done = chunk\n",
    "        else:\n",
    "            done = pd.concat([done, chunk], ignore_index=True)\n",
    "\n",
    "        done.to_csv(CHECKPOINT_CSV, index=False)\n",
    "        done_paths = set(done[\"image_path\"].astype(str).tolist())\n",
    "        print(\"Checkpoint saved:\", len(done_paths))\n",
    "\n",
    "# final save\n",
    "if out_rows:\n",
    "    chunk = pd.DataFrame(out_rows)\n",
    "    done = pd.concat([done, chunk], ignore_index=True)\n",
    "\n",
    "done.to_csv(CSV_OUT, index=False)\n",
    "print(\"Saved final:\", CSV_OUT, \"| rows:\", len(done))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3534b3-c1a4-455a-881e-1625468e62fc",
   "metadata": {},
   "source": [
    "Plots for all demographic attributes (face / gender / age_group / race / skin_tone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43529274-c3e8-4aaa-bfcd-0630b68281c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CSV_OUT = \"data/laion_big_light_tau1_0.2989_fairface.csv\"\n",
    "df_ff = pd.read_csv(CSV_OUT)\n",
    "\n",
    "print(\"Rows:\", len(df_ff))\n",
    "print(\"Face detected %:\", df_ff[\"face_detected\"].mean() * 100)\n",
    "\n",
    "def barplot(col, title=None, top_n=None):\n",
    "    if col not in df_ff.columns:\n",
    "        print(f\"❌ Missing column: {col}\")\n",
    "        return\n",
    "\n",
    "    s = df_ff[col].dropna()\n",
    "    if s.empty:\n",
    "        print(f\"⚠️ No values available for: {col} (all NaN)\")\n",
    "        return\n",
    "\n",
    "    counts = s.value_counts()\n",
    "    if top_n is not None:\n",
    "        counts = counts.head(top_n)\n",
    "\n",
    "    ax = counts.plot(kind=\"bar\", title=title or f\"{col} distribution\")\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1) Face detected (including False/True)\n",
    "if \"face_detected\" in df_ff.columns:\n",
    "    face_counts = df_ff[\"face_detected\"].fillna(False).value_counts()\n",
    "    ax = face_counts.plot(kind=\"bar\", title=\"Face Detection (True/False)\")\n",
    "    ax.set_xlabel(\"face_detected\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2) Gender\n",
    "barplot(\"gender\", \"Gender Distribution\")\n",
    "\n",
    "# 3) Age group\n",
    "barplot(\"age_group\", \"Age Group Distribution\")\n",
    "\n",
    "# 4) Race\n",
    "barplot(\"race\", \"Race Distribution\")\n",
    "\n",
    "# 5) Skin tone (your proxy mapping)\n",
    "barplot(\"skin_tone\", \"Skin Tone (Proxy) Distribution\")\n",
    "\n",
    "# Optional: how many rows have each attribute filled\n",
    "cols = [\"face_detected\", \"gender\", \"age_group\", \"race\", \"skin_tone\"]\n",
    "print(\"\\nFilled ratios:\")\n",
    "for c in cols:\n",
    "    if c in df_ff.columns:\n",
    "        print(f\"- {c}: {df_ff[c].notna().mean()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7a79f-862e-4463-836a-65e744b8d8fa",
   "metadata": {},
   "source": [
    "JSON summary (counts + percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac19fe5-4eb6-4c13-bfb6-89e000424bef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "CSV_OUT = \"data/laion_big_light_tau1_0.2989_fairface.csv\"\n",
    "JSON_OUT = \"data/week5_fairface_light_summary.json\"\n",
    "\n",
    "df_ff = pd.read_csv(CSV_OUT)\n",
    "\n",
    "def vc(col):\n",
    "    if col not in df_ff.columns:\n",
    "        return None\n",
    "    s = df_ff[col].dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    return s.value_counts().to_dict()\n",
    "\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"input_csv\": CSV_OUT,\n",
    "    \"n_rows\": int(len(df_ff)),\n",
    "\n",
    "    \"face_detected\": {\n",
    "        \"true_count\": int((df_ff[\"face_detected\"] == True).sum()) if \"face_detected\" in df_ff.columns else None,\n",
    "        \"false_count\": int((df_ff[\"face_detected\"] == False).sum()) if \"face_detected\" in df_ff.columns else None,\n",
    "        \"true_pct\": float((df_ff[\"face_detected\"] == True).mean()*100) if \"face_detected\" in df_ff.columns else None,\n",
    "    },\n",
    "\n",
    "    \"filled_ratio_pct\": {\n",
    "        c: float(df_ff[c].notna().mean()*100) if c in df_ff.columns else None\n",
    "        for c in [\"gender\", \"age_group\", \"race\", \"skin_tone\"]\n",
    "    },\n",
    "\n",
    "    \"distributions\": {\n",
    "        \"gender\": vc(\"gender\"),\n",
    "        \"age_group\": vc(\"age_group\"),\n",
    "        \"race\": vc(\"race\"),\n",
    "        \"skin_tone\": vc(\"skin_tone\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(JSON_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"✅ Saved JSON summary:\", JSON_OUT)\n",
    "print(json.dumps(summary, indent=2)[:1200], \"...\\n\")  # preview\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
